{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl-VjNdlkq3O",
        "outputId": "e84a59df-5f7f-463d-a8d4-8b70c260a094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.32.3)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
            "Downloading speechbrain-1.0.1-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.2/807.2 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 speechbrain-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install assemblyai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RmlheGrk6cO",
        "outputId": "4b4dc623-96de-4cbc-cd4c-15410dbf3c21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting assemblyai\n",
            "  Downloading assemblyai-0.33.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting httpx>=0.19.0 (from assemblyai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic!=1.10.7,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (4.12.2)\n",
            "Collecting websockets>=11.0 (from assemblyai)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.19.0->assemblyai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.19.0->assemblyai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (2.23.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.19.0->assemblyai) (1.2.2)\n",
            "Downloading assemblyai-0.33.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, h11, httpcore, httpx, assemblyai\n",
            "Successfully installed assemblyai-0.33.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 websockets-13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZi8DralH5B",
        "outputId": "d3aa4bb7-1eb5-421e-b58f-3a472d867189"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade speechbrain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5I7DBdols-7",
        "outputId": "06ee9eb1-7bff-44ec-efc4-657a010bf37a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.32.3)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain) (0.18.6)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "from speechbrain.inference import EncoderClassifier\n"
      ],
      "metadata": {
        "id": "wzuEPtNfkwFC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"dhjsg\""
      ],
      "metadata": {
        "id": "HlOABwS0k_yh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_audio(file_path):\n",
        "    headers = {\n",
        "        'authorization': api_key,\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = requests.post('https://api.assemblyai.com/v2/upload', headers=headers, files={'file': f})\n",
        "\n",
        "    return response.json()['upload_url']"
      ],
      "metadata": {
        "id": "YPHuIYJQmKes"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Request transcription with speaker diarization enabled\n",
        "def request_transcription(audio_url):\n",
        "    endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
        "\n",
        "    json_data = {\n",
        "        \"audio_url\": audio_url,\n",
        "        \"speaker_diarization\": True,\n",
        "        \"punctuate\": False,\n",
        "        \"format_text\": False\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"authorization\": api_key,\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(endpoint, json=json_data, headers=headers)\n",
        "\n",
        "    # Check for errors in the response\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error in transcription request:\", response.json())\n",
        "        raise Exception(\"Failed to request transcription.\")\n",
        "\n",
        "    return response.json()['id']"
      ],
      "metadata": {
        "id": "tn5IGnCymLJu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_status(transcript_id):\n",
        "    endpoint = f\"https://api.assemblyai.com/v2/transcript/{transcript_id}\"\n",
        "\n",
        "    headers = {\n",
        "        \"authorization\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(endpoint, headers=headers)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "E18cn5yXmPzR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_speaker_timestamps(transcript_id):\n",
        "    while True:\n",
        "        result = check_status(transcript_id)\n",
        "        if result['status'] == 'completed':\n",
        "            # Parse speaker timestamps from the \"utterances\" key\n",
        "            speaker_segments = result.get('utterances', [])\n",
        "            speaker_timestamps = [\n",
        "                {\n",
        "                    \"speaker\": segment[\"speaker\"],\n",
        "                    \"start_time\": segment[\"start\"],\n",
        "                    \"end_time\": segment[\"end\"]\n",
        "                }\n",
        "                for segment in speaker_segments\n",
        "            ]\n",
        "            return speaker_timestamps\n",
        "        elif result['status'] == 'failed':\n",
        "            raise Exception(\"Transcription failed.\")\n",
        "        else:\n",
        "            print(\"Processing... Please wait.\")\n",
        "            time.sleep(5)"
      ],
      "metadata": {
        "id": "vwZE66DMmSbr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_audio_segments(timestamps, audio):\n",
        "    speaker_audio_segments = {}\n",
        "\n",
        "    for segment in timestamps:\n",
        "        start_time = segment[\"start_time\"] * 1000  # convert to ms\n",
        "        end_time = segment[\"end_time\"] * 1000      # convert to ms\n",
        "        speaker = segment[\"speaker\"]\n",
        "\n",
        "        speaker_segment = audio[start_time:end_time]\n",
        "\n",
        "        if speaker not in speaker_audio_segments:\n",
        "            speaker_audio_segments[speaker] = [speaker_segment]\n",
        "        else:\n",
        "            speaker_audio_segments[speaker].append(speaker_segment)\n",
        "\n",
        "    return speaker_audio_segments\n"
      ],
      "metadata": {
        "id": "gOh7Bup2mWTR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Perform emotion analysis using speechbrain\n",
        "def analyze_emotion(audio_segment, classifier):\n",
        "    # Convert pydub AudioSegment to a numpy array\n",
        "    samples = np.array(audio_segment.get_array_of_samples())\n",
        "    samples = samples.astype(np.float32) / (2 ** 15)  # Normalize to -1 to 1\n",
        "\n",
        "    # Save the audio segment temporarily as a WAV file\n",
        "    temp_audio_file = \"temp_audio.wav\"\n",
        "    audio_segment.export(temp_audio_file, format=\"wav\")\n",
        "\n",
        "    # Run the emotion classifier on the temporary file\n",
        "    emotion_prediction = classifier.classify_file(temp_audio_file)\n",
        "    return emotion_prediction"
      ],
      "metadata": {
        "id": "Ct7c4w-KmcV1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to handle the entire process\n",
        "def main(file_path):\n",
        "    # Step 1: Upload the audio file\n",
        "    audio_url = upload_audio(file_path)\n",
        "\n",
        "    # Step 2: Request transcription with speaker diarization\n",
        "    transcript_id = request_transcription(audio_url)\n",
        "\n",
        "    # Step 3: Retrieve speaker timestamps\n",
        "    speaker_timestamps = get_speaker_timestamps(transcript_id)\n",
        "\n",
        "    # Step 4: Load the audio using pydub\n",
        "    audio = AudioSegment.from_mp3(file_path)\n",
        "\n",
        "    # Step 5: Split the audio into segments for each speaker\n",
        "    speaker_segments = split_audio_segments(speaker_timestamps, audio)\n",
        "\n",
        "    # Step 6: Load the emotion recognition model from speechbrain\n",
        "    classifier = EncoderClassifier.from_hparams(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"tmp\")\n",
        "\n",
        "    # Step 7: Perform emotion analysis for each speaker's segments\n",
        "    for speaker, segments in speaker_segments.items():\n",
        "        for segment in segments:\n",
        "            emotion = analyze_emotion(segment, classifier)\n",
        "            print(f\"Emotion for {speaker}: {emotion}\")\n"
      ],
      "metadata": {
        "id": "ZHHgWQCvmenw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"  # Replace with your audio file path\n",
        "    main(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "xEo7-6iSm6jx",
        "outputId": "ef897a72-4621-40b0-a3da-4f9c6c891a2b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in transcription request: {'error': 'Invalid endpoint schema, please refer to documentation for examples.'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Failed to request transcription.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-779808f0acf8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"\u001b[0m  \u001b[0;31m# Replace with your audio file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-d75d201e3c67>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Step 2: Request transcription with speaker diarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtranscript_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_transcription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Step 3: Retrieve speaker timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-bc2fe7c957aa>\u001b[0m in \u001b[0;36mrequest_transcription\u001b[0;34m(audio_url)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error in transcription request:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to request transcription.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Failed to request transcription."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNkVv5s9nDuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######"
      ],
      "metadata": {
        "id": "yIjIZ0RSoBIq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from speechbrain.inference import EncoderClassifier\n",
        "\n",
        "# Set your API key\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a local file path or a publicly accessible URL\n",
        "audio_file = \"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"  # Replace with your audio file path\n",
        "\n",
        "# Configure the transcription settings to enable speaker labels\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        ")\n",
        "\n",
        "# Configure the transcription settings to enable speaker labels\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        ")\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Load the audio for segmentation\n",
        "audio = AudioSegment.from_mp3(audio_file)\n",
        "\n",
        "# Prepare the emotion classifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"tmp\")\n",
        "\n",
        "# Analyze emotion for each utterance\n",
        "for utterance in transcript.utterances:\n",
        "    start_time = int(utterance.start)  # Start time in milliseconds\n",
        "    end_time = int(utterance.end)      # End time in milliseconds\n",
        "    speaker = utterance.speaker\n",
        "\n",
        "    # Extract the segment\n",
        "    segment = audio[start_time:end_time]\n",
        "\n",
        "    # Save the segment to a temporary file for emotion analysis\n",
        "    temp_audio_file = \"temp_audio.wav\"\n",
        "    segment.export(temp_audio_file, format=\"wav\")\n",
        "\n",
        "\n",
        "    # Analyze emotion\n",
        "    # Directly classify the audio file\n",
        "    emotion_prediction = classifier.classify_file(temp_audio_file)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Speaker {speaker}: {utterance.text}\")\n",
        "    print(f\"Emotion for Speaker {speaker}: {emotion_prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "aijIboEdoD2P",
        "outputId": "d7677f49-8be0-4d97-8564-340f5761d5db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:speechbrain.lobes.models.huggingface_transformers.huggingface:speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.\n",
            "/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ModuleDict' object has no attribute 'compute_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f1f663ad62b7>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Analyze emotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Directly classify the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0memotion_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_audio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/classifiers.py\u001b[0m in \u001b[0;36mclassify_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mrel_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mout_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/classifiers.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Computing features and embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_var_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ModuleDict' object has no attribute 'compute_features'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "from speechbrain.inference import EncoderClassifier\n",
        "\n",
        "# Your AssemblyAI API key\n",
        "api_key = \"f69817768510455f96f4088d628065fb\"  # Replace with your actual API key\n",
        "\n",
        "# Step 1: Upload the audio file to AssemblyAI\n",
        "def upload_audio(file_path):\n",
        "    headers = {\n",
        "        'authorization': api_key,\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = requests.post('https://api.assemblyai.com/v2/upload', headers=headers, files={'file': f})\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error uploading audio:\", response.json())\n",
        "        raise Exception(\"Failed to upload audio.\")\n",
        "\n",
        "    return response.json()['upload_url']\n",
        "\n",
        "\n",
        "def request_transcription(audio_url):\n",
        "    endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
        "\n",
        "    json_data = {\n",
        "        \"audio_url\": audio_url,\n",
        "        \"speaker_diarization\": True,\n",
        "        \"punctuate\": True,\n",
        "        \"format_text\": True\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"authorization\": api_key,\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    print(\"Audio URL:\", audio_url)  # Log the audio URL\n",
        "    print(\"Request JSON:\", json_data)  # Log the JSON payload\n",
        "\n",
        "    response = requests.post(endpoint, json=json_data, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error in transcription request:\", response.json())\n",
        "        raise Exception(f\"Failed to request transcription: {response.json()}\")  # Include response in exception\n",
        "\n",
        "    return response.json().get('id', None)\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Check transcription status and retrieve timestamps\n",
        "def check_status(transcript_id):\n",
        "    endpoint = f\"https://api.assemblyai.com/v2/transcript/{transcript_id}\"\n",
        "\n",
        "    headers = {\n",
        "        \"authorization\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(endpoint, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# Step 4: Extract speaker timestamps\n",
        "def get_speaker_timestamps(transcript_id):\n",
        "    while True:\n",
        "        result = check_status(transcript_id)\n",
        "        if result['status'] == 'completed':\n",
        "            speaker_segments = result.get('utterances', [])\n",
        "            speaker_timestamps = [\n",
        "                {\n",
        "                    \"speaker\": segment[\"speaker\"],\n",
        "                    \"start_time\": segment[\"start\"],\n",
        "                    \"end_time\": segment[\"end\"]\n",
        "                }\n",
        "                for segment in speaker_segments\n",
        "            ]\n",
        "            return speaker_timestamps\n",
        "        elif result['status'] == 'failed':\n",
        "            print(\"Transcription failed:\", result)\n",
        "            raise Exception(\"Transcription failed.\")\n",
        "        else:\n",
        "            print(\"Processing... Please wait.\")\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "# Step 5: Split the audio file into segments based on speaker timestamps\n",
        "def split_audio_segments(timestamps, audio):\n",
        "    speaker_audio_segments = {}\n",
        "\n",
        "    for segment in timestamps:\n",
        "        start_time = segment[\"start_time\"] * 1000  # convert to ms\n",
        "        end_time = segment[\"end_time\"] * 1000      # convert to ms\n",
        "        speaker = segment[\"speaker\"]\n",
        "\n",
        "        speaker_segment = audio[start_time:end_time]\n",
        "\n",
        "        if speaker not in speaker_audio_segments:\n",
        "            speaker_audio_segments[speaker] = [speaker_segment]\n",
        "        else:\n",
        "            speaker_audio_segments[speaker].append(speaker_segment)\n",
        "\n",
        "    return speaker_audio_segments\n",
        "\n",
        "\n",
        "# Step 6: Perform emotion analysis using speechbrain\n",
        "def analyze_emotion(audio_segment, classifier):\n",
        "    samples = np.array(audio_segment.get_array_of_samples())\n",
        "    samples = samples.astype(np.float32) / (2 ** 15)\n",
        "\n",
        "    temp_audio_file = \"temp_audio.wav\"\n",
        "    audio_segment.export(temp_audio_file, format=\"wav\")\n",
        "\n",
        "    emotion_prediction = classifier.classify_file(temp_audio_file)\n",
        "    return emotion_prediction\n",
        "\n",
        "\n",
        "# Main function to handle the entire process\n",
        "def main(file_path):\n",
        "    audio_url = upload_audio(file_path)\n",
        "\n",
        "    transcript_id = request_transcription(audio_url)\n",
        "\n",
        "    speaker_timestamps = get_speaker_timestamps(transcript_id)\n",
        "\n",
        "    audio = AudioSegment.from_mp3(file_path)\n",
        "\n",
        "    speaker_segments = split_audio_segments(speaker_timestamps, audio)\n",
        "\n",
        "    classifier = EncoderClassifier.from_hparams(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"tmp\")\n",
        "\n",
        "    for speaker, segments in speaker_segments.items():\n",
        "        for segment in segments:\n",
        "            emotion = analyze_emotion(segment, classifier)\n",
        "            print(f\"Emotion for {speaker}: {emotion}\")\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"  # Replace with your audio file path\n",
        "    main(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "3ekpxpaPoLtJ",
        "outputId": "1b507079-8b01-4fc8-9f85-2e3bf0c45cfd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio URL: https://cdn.assemblyai.com/upload/c729fa13-d1d2-45f2-bf79-d5650a8f420b\n",
            "Request JSON: {'audio_url': 'https://cdn.assemblyai.com/upload/c729fa13-d1d2-45f2-bf79-d5650a8f420b', 'speaker_diarization': True, 'punctuate': True, 'format_text': True}\n",
            "Error in transcription request: {'error': 'Invalid endpoint schema, please refer to documentation for examples.'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Failed to request transcription: {'error': 'Invalid endpoint schema, please refer to documentation for examples.'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-eb90be43b645>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"\u001b[0m  \u001b[0;31m# Replace with your audio file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-eb90be43b645>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0maudio_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtranscript_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_transcription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mspeaker_timestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_speaker_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-eb90be43b645>\u001b[0m in \u001b[0;36mrequest_transcription\u001b[0;34m(audio_url)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error in transcription request:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to request transcription: {response.json()}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Include response in exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Failed to request transcription: {'error': 'Invalid endpoint schema, please refer to documentation for examples.'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# Step 1: Set up your API key and base URL\n",
        "api_key = \"f69817768510455f96f4088d628065fb\"  # Replace with your actual API key\n",
        "base_url = \"https://api.assemblyai.com/v2\"\n",
        "\n",
        "# Step 2: Set up the headers for the API requests\n",
        "headers = {\n",
        "    \"authorization\": api_key\n",
        "}\n",
        "\n",
        "# Step 3: Upload your audio file\n",
        "def upload_audio(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        response = requests.post(base_url + \"/upload\", headers=headers, data=f)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error uploading audio:\", response.json())\n",
        "        raise Exception(\"Failed to upload audio.\")\n",
        "\n",
        "    return response.json()[\"upload_url\"]\n",
        "\n",
        "# Step 4: Request transcription using the uploaded audio URL\n",
        "def request_transcription(upload_url):\n",
        "    data = {\n",
        "        \"audio_url\": upload_url\n",
        "    }\n",
        "\n",
        "    response = requests.post(base_url + \"/transcript\", json=data, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error in transcription request:\", response.json())\n",
        "        raise Exception(\"Failed to request transcription.\")\n",
        "\n",
        "    return response.json()[\"id\"]\n",
        "\n",
        "# Step 5: Poll the API for the transcription result\n",
        "def poll_transcription(transcript_id):\n",
        "    polling_endpoint = f\"{base_url}/transcript/{transcript_id}\"\n",
        "\n",
        "    while True:\n",
        "        transcription_result = requests.get(polling_endpoint, headers=headers).json()\n",
        "\n",
        "        if transcription_result['status'] == 'completed':\n",
        "            return transcription_result\n",
        "        elif transcription_result['status'] == 'error':\n",
        "            raise RuntimeError(f\"Transcription failed: {transcription_result['error']}\")\n",
        "        else:\n",
        "            time.sleep(3)\n",
        "\n",
        "# Main function to handle the workflow\n",
        "def main(file_path):\n",
        "    try:\n",
        "        # Upload audio file\n",
        "        upload_url = upload_audio(file_path)\n",
        "\n",
        "        # Request transcription\n",
        "        transcript_id = request_transcription(upload_url)\n",
        "\n",
        "        # Poll for transcription result\n",
        "        result = poll_transcription(transcript_id)\n",
        "\n",
        "        # Print the transcription text\n",
        "        print(\"Transcription Text:\", result['text'])\n",
        "        print(\"Confidence Score:\", result['confidence'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    audio_file_path = \"/content/ElevenLabs_2024-09-17T17_23_57_Laura_pre_s50_sb75_se0_b_m2.mp3\"  # Replace with your audio file path\n",
        "    main(audio_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X7fXZHBq240",
        "outputId": "54bca7d3-46fd-439e-dcf8-05bcb0426f88"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription Text: And finally, theres the negative prompt box. This is where you can specify what you dont want in your video. So, for example, if I dont want the puppy to interact with any other animals, I could mention that here to avoid distractions.\n",
            "Confidence Score: 0.940107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "\n",
        "# Set your API key\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a publicly-accessible audio URL or a local file\n",
        "audio_file = (\n",
        "    \"https://github.com/AssemblyAI-Community/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3\"\n",
        ")\n",
        "\n",
        "# Configure the transcription with speaker labels and sentiment analysis\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        "    sentiment_analysis=True\n",
        ")\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Extract and display results\n",
        "for utterance in transcript.utterances:\n",
        "    # Print speaker and their spoken text\n",
        "    print(f\"Speaker {utterance.speaker}: {utterance.text}\")\n",
        "\n",
        "    # Extract sentiment analysis for each utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            print(f\"Speaker {utterance.speaker} - Sentiment: {sentiment_result.sentiment} \"\n",
        "                  f\"(Confidence: {sentiment_result.confidence}) \"\n",
        "                  f\"Timestamp: {sentiment_result.start} - {sentiment_result.end}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pJv7O7Grrxe",
        "outputId": "cf579d96-3afe-42d7-cc25-5eb5db816261"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why. So he called Peter DiCarlo, an associate professor in the department of Environmental Health and Engineering at Johns Hopkins University. Good morning. Professor.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.818103) Timestamp: 240 - 6262\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.5900679) Timestamp: 6446 - 10998\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.537145) Timestamp: 11094 - 15734\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.79514164) Timestamp: 15902 - 18390\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9258168) Timestamp: 18430 - 25334\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 25502 - 26006\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.6240708) Timestamp: 26038 - 26770\n",
            "Speaker B: Good morning.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 27850 - 28786\n",
            "Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.7462095) Timestamp: 28938 - 37350\n",
            "Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those canadian wildfires through Pennsylvania into the mid Atlantic in the northeast and kind of just dropping the smoke there.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.73488915) Timestamp: 39050 - 40510\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.61490905) Timestamp: 40930 - 56104\n",
            "Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is harmful.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.8751137) Timestamp: 56282 - 59284\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.80006605) Timestamp: 59332 - 60920\n",
            "Speaker B: It is. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic, smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological, your brain.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62340 - 62884\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62932 - 63332\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9444374) Timestamp: 63396 - 66920\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.7673307) Timestamp: 67740 - 82800\n",
            "Speaker A: What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.93296945) Timestamp: 83420 - 85460\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8246211) Timestamp: 85500 - 87964\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8175228) Timestamp: 88012 - 89244\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8153341) Timestamp: 89372 - 90316\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8712161) Timestamp: 90348 - 92400\n",
            "Speaker B: Yeah. So the concentration of particulate matter I was looking at, some of the monitors that we have was reaching levels of what are in science peak 150 micrograms per meter cubed, which is more than ten times what the annual average should be in about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.54914486) Timestamp: 93460 - 93860\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6478079) Timestamp: 93900 - 113244\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.49572414) Timestamp: 113372 - 119628\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9395254) Timestamp: 119724 - 123380\n",
            "Speaker A: And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.53734136) Timestamp: 123460 - 124900\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.779357) Timestamp: 124980 - 128628\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.48532158) Timestamp: 128684 - 132828\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5655255) Timestamp: 132884 - 136360\n",
            "Speaker B: It's the youngest. So children, obviously, whose bodies are still developing, the elderly, who are, you know, their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have pre existing health conditions, people with respiratory conditions or heart conditions, can be triggered by high levels of air pollution.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.78433657) Timestamp: 137440 - 138704\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.890515) Timestamp: 138752 - 149620\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8824855) Timestamp: 150560 - 156780\n",
            "Speaker A: Could this get worse?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.9435272) Timestamp: 157360 - 158860\n",
            "Speaker B: That's a good question. I mean, I think if, and in some areas it's much worse than others, and it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.828334) Timestamp: 162120 - 163112\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8275031) Timestamp: 163216 - 170320\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7581229) Timestamp: 171020 - 176892\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7974404) Timestamp: 177076 - 183652\n",
            "Speaker A: I was going to ask you about.\n",
            "Speaker B: More fires start burning. I don't expect the concentrations to go up too much higher.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.5720193) Timestamp: 185644 - 189060\n",
            "Speaker A: I was going to ask you how, and you started to answer this, but how much longer could this last? Forgive me if I'm asking you to speculate, but what do you think?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5477228) Timestamp: 189140 - 193364\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.7086924) Timestamp: 193452 - 196480\n",
            "Speaker B: Well, I think the fires are going to burn for a little bit longer, but the key for us in the US is the weather system changing. And so right now it's kind of the weather systems that are pulling that air into our mid Atlantic and northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6709003) Timestamp: 198080 - 203720\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8217785) Timestamp: 203760 - 211048\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6655182) Timestamp: 211144 - 219088\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6444754) Timestamp: 219144 - 220952\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8290641) Timestamp: 220976 - 227616\n",
            "Speaker A: And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.69644165) Timestamp: 227728 - 232328\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5142844) Timestamp: 232464 - 240100\n",
            "Speaker B: I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the western Us. So the eastern us getting hit right now is a little bit new. But, yeah, I think with climate change moving forward, this is something that is going to happen more frequently.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6701288) Timestamp: 241320 - 245080\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.4755036) Timestamp: 245120 - 251136\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6273222) Timestamp: 251168 - 255592\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.58344245) Timestamp: 255696 - 258040\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.66216063) Timestamp: 258160 - 261700\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6574885) Timestamp: 262160 - 267792\n",
            "Speaker A: That's Peter DiCarlo, associate professor in the department of Environmental Health and engineering at Johns Hopkins University. Professor de Carlo, thanks so much for joining us and sharing this expertise with us.\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9091453) Timestamp: 267976 - 274576\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.97781634) Timestamp: 274688 - 278460\n",
            "Speaker B: Thank you for having me.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.97090584) Timestamp: 279440 - 280280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "\n",
        "# Set your API key\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a publicly-accessible audio URL or a local file\n",
        "audio_file = (\n",
        "    \"https://github.com/AssemblyAI-Community/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3\"\n",
        ")\n",
        "\n",
        "# Configure the transcription with speaker labels and sentiment analysis\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        "    sentiment_analysis=True\n",
        ")\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Initialize sentiment counters for each speaker dynamically\n",
        "speaker_sentiments = {}\n",
        "\n",
        "# Extract and display results\n",
        "for utterance in transcript.utterances:\n",
        "    speaker_id = utterance.speaker  # e.g., 'A' or 'B'\n",
        "\n",
        "    # Initialize counters for the speaker if not already present\n",
        "    if speaker_id not in speaker_sentiments:\n",
        "        speaker_sentiments[speaker_id] = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "\n",
        "    # Print speaker and their spoken text\n",
        "    print(f\"Speaker {speaker_id}: {utterance.text}\")\n",
        "\n",
        "    # Extract sentiment analysis for each utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            sentiment = sentiment_result.sentiment\n",
        "\n",
        "            # Update sentiment counters for the corresponding speaker\n",
        "            if sentiment == \"POSITIVE\":\n",
        "                speaker_sentiments[speaker_id]['positive'] += 1\n",
        "            elif sentiment == \"NEGATIVE\":\n",
        "                speaker_sentiments[speaker_id]['negative'] += 1\n",
        "            elif sentiment == \"NEUTRAL\":\n",
        "                speaker_sentiments[speaker_id]['neutral'] += 1\n",
        "\n",
        "            print(f\"Speaker {speaker_id} - Sentiment: {sentiment} \"\n",
        "                  f\"(Confidence: {sentiment_result.confidence}) \"\n",
        "                  f\"Timestamp: {sentiment_result.start} - {sentiment_result.end}\")\n",
        "\n",
        "# Print overall sentiment for each speaker\n",
        "print(\"\\nOverall Sentiment Analysis:\")\n",
        "for speaker, sentiments in speaker_sentiments.items():\n",
        "    total = sum(sentiments.values())\n",
        "    overall_sentiment = max(sentiments, key=sentiments.get)\n",
        "    print(f\"{speaker}: {overall_sentiment.capitalize()} (Counts - Positive: {sentiments['positive']}, \"\n",
        "          f\"Negative: {sentiments['negative']}, Neutral: {sentiments['neutral']})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdoel9egtIAO",
        "outputId": "0e8ff55a-8718-49e0-89b3-ed9780e9f1f9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why. So he called Peter DiCarlo, an associate professor in the department of Environmental Health and Engineering at Johns Hopkins University. Good morning. Professor.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.818103) Timestamp: 240 - 6262\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.5900679) Timestamp: 6446 - 10998\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.537145) Timestamp: 11094 - 15734\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.79514164) Timestamp: 15902 - 18390\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9258168) Timestamp: 18430 - 25334\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 25502 - 26006\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.6240708) Timestamp: 26038 - 26770\n",
            "Speaker B: Good morning.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 27850 - 28786\n",
            "Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.7462095) Timestamp: 28938 - 37350\n",
            "Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those canadian wildfires through Pennsylvania into the mid Atlantic in the northeast and kind of just dropping the smoke there.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.73488915) Timestamp: 39050 - 40510\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.61490905) Timestamp: 40930 - 56104\n",
            "Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is harmful.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.8751137) Timestamp: 56282 - 59284\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.80006605) Timestamp: 59332 - 60920\n",
            "Speaker B: It is. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic, smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological, your brain.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62340 - 62884\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62932 - 63332\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9444374) Timestamp: 63396 - 66920\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.7673307) Timestamp: 67740 - 82800\n",
            "Speaker A: What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.93296945) Timestamp: 83420 - 85460\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8246211) Timestamp: 85500 - 87964\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8175228) Timestamp: 88012 - 89244\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8153341) Timestamp: 89372 - 90316\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8712161) Timestamp: 90348 - 92400\n",
            "Speaker B: Yeah. So the concentration of particulate matter I was looking at, some of the monitors that we have was reaching levels of what are in science peak 150 micrograms per meter cubed, which is more than ten times what the annual average should be in about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.54914486) Timestamp: 93460 - 93860\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6478079) Timestamp: 93900 - 113244\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.49572414) Timestamp: 113372 - 119628\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9395254) Timestamp: 119724 - 123380\n",
            "Speaker A: And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.53734136) Timestamp: 123460 - 124900\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.779357) Timestamp: 124980 - 128628\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.48532158) Timestamp: 128684 - 132828\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5655255) Timestamp: 132884 - 136360\n",
            "Speaker B: It's the youngest. So children, obviously, whose bodies are still developing, the elderly, who are, you know, their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have pre existing health conditions, people with respiratory conditions or heart conditions, can be triggered by high levels of air pollution.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.78433657) Timestamp: 137440 - 138704\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.890515) Timestamp: 138752 - 149620\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8824855) Timestamp: 150560 - 156780\n",
            "Speaker A: Could this get worse?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.9435272) Timestamp: 157360 - 158860\n",
            "Speaker B: That's a good question. I mean, I think if, and in some areas it's much worse than others, and it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.828334) Timestamp: 162120 - 163112\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8275031) Timestamp: 163216 - 170320\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7581229) Timestamp: 171020 - 176892\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7974404) Timestamp: 177076 - 183652\n",
            "Speaker A: I was going to ask you about.\n",
            "Speaker B: More fires start burning. I don't expect the concentrations to go up too much higher.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.5720193) Timestamp: 185644 - 189060\n",
            "Speaker A: I was going to ask you how, and you started to answer this, but how much longer could this last? Forgive me if I'm asking you to speculate, but what do you think?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5477228) Timestamp: 189140 - 193364\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.7086924) Timestamp: 193452 - 196480\n",
            "Speaker B: Well, I think the fires are going to burn for a little bit longer, but the key for us in the US is the weather system changing. And so right now it's kind of the weather systems that are pulling that air into our mid Atlantic and northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6709003) Timestamp: 198080 - 203720\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8217785) Timestamp: 203760 - 211048\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6655182) Timestamp: 211144 - 219088\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6444754) Timestamp: 219144 - 220952\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8290641) Timestamp: 220976 - 227616\n",
            "Speaker A: And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.69644165) Timestamp: 227728 - 232328\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5142844) Timestamp: 232464 - 240100\n",
            "Speaker B: I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the western Us. So the eastern us getting hit right now is a little bit new. But, yeah, I think with climate change moving forward, this is something that is going to happen more frequently.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6701288) Timestamp: 241320 - 245080\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.4755036) Timestamp: 245120 - 251136\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6273222) Timestamp: 251168 - 255592\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.58344245) Timestamp: 255696 - 258040\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.66216063) Timestamp: 258160 - 261700\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6574885) Timestamp: 262160 - 267792\n",
            "Speaker A: That's Peter DiCarlo, associate professor in the department of Environmental Health and engineering at Johns Hopkins University. Professor de Carlo, thanks so much for joining us and sharing this expertise with us.\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9091453) Timestamp: 267976 - 274576\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.97781634) Timestamp: 274688 - 278460\n",
            "Speaker B: Thank you for having me.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.97090584) Timestamp: 279440 - 280280\n",
            "\n",
            "Overall Sentiment Analysis:\n",
            "A: Neutral (Counts - Positive: 2, Negative: 9, Neutral: 15)\n",
            "B: Neutral (Counts - Positive: 3, Negative: 7, Neutral: 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "# Set your API key\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a publicly-accessible audio URL or a local file\n",
        "audio_file = (\n",
        "    \"/content/20230607_me_canadian_wildfires.mp3\"\n",
        ")\n",
        "\n",
        "# Configure the transcription with speaker labels and sentiment analysis\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        "    sentiment_analysis=True\n",
        ")\n",
        "\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Load the pretrained model and processor for emotion recognition\n",
        "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "emotion_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "\n",
        "# # Function to classify emotions from audio\n",
        "# def classify_emotion(audio_path):\n",
        "#     audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "#     inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         logits = emotion_model(**inputs).logits\n",
        "\n",
        "#     predicted_ids = torch.argmax(logits, dim=-1)\n",
        "#     return predicted_ids.item()  # Adjust based on your model's output format\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# # Function to classify emotions from audio\n",
        "# def classify_emotion(audio_path):\n",
        "#     audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "#     # Check if audio length is less than the minimum required length (adjust as needed)\n",
        "#     min_length = emotion_processor.config.input_features[0]['sequence_length']  # Example to get the required length\n",
        "#     if len(audio_input) < min_length:\n",
        "#         # Pad the audio with zeros (silence) if it's too short\n",
        "#         padding_length = min_length - len(audio_input)\n",
        "#         audio_input = np.pad(audio_input, (0, padding_length), 'constant')\n",
        "\n",
        "#     inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         logits = emotion_model(**inputs).logits\n",
        "\n",
        "#     predicted_ids = torch.argmax(logits, dim=-1)\n",
        "#     return predicted_ids.item()  # Adjust based on your model's output format\n",
        "\n",
        "def classify_emotion(audio_path):\n",
        "    audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Define a common input length for Wav2Vec2 models, usually 2 seconds at 16kHz\n",
        "    target_length = 32000  # 2 seconds of audio at 16000 Hz\n",
        "\n",
        "    # Check if audio length is less than the target length\n",
        "    if len(audio_input) < target_length:\n",
        "        # Pad the audio with zeros (silence) if it's too short\n",
        "        padding_length = target_length - len(audio_input)\n",
        "        audio_input = np.pad(audio_input, (0, padding_length), 'constant')\n",
        "    else:\n",
        "        # Trim the audio if it's too long\n",
        "        audio_input = audio_input[:target_length]\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = emotion_model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    return predicted_ids.item()  # Adjust based on your model's output format\n",
        "\n",
        "# Initialize sentiment counters for each speaker\n",
        "speaker_sentiments = {}\n",
        "speaker_emotions = {}\n",
        "\n",
        "# Process each utterance\n",
        "for utterance in transcript.utterances:\n",
        "    speaker_id = utterance.speaker  # e.g., 'A' or 'B'\n",
        "\n",
        "    # Initialize counters for the speaker if not already present\n",
        "    if speaker_id not in speaker_sentiments:\n",
        "        speaker_sentiments[speaker_id] = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "        speaker_emotions[speaker_id] = []\n",
        "\n",
        "    # Print speaker and their spoken text\n",
        "    print(f\"Speaker {speaker_id}: {utterance.text}\")\n",
        "\n",
        "    # Extract sentiment analysis for each utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            sentiment = sentiment_result.sentiment\n",
        "\n",
        "            # Update sentiment counters for the corresponding speaker\n",
        "            if sentiment == \"POSITIVE\":\n",
        "                speaker_sentiments[speaker_id]['positive'] += 1\n",
        "            elif sentiment == \"NEGATIVE\":\n",
        "                speaker_sentiments[speaker_id]['negative'] += 1\n",
        "            elif sentiment == \"NEUTRAL\":\n",
        "                speaker_sentiments[speaker_id]['neutral'] += 1\n",
        "\n",
        "            print(f\"Speaker {speaker_id} - Sentiment: {sentiment} \"\n",
        "                  f\"(Confidence: {sentiment_result.confidence}) \"\n",
        "                  f\"Timestamp: {sentiment_result.start} - {sentiment_result.end}\")\n",
        "\n",
        "    # Classify emotion for the utterance\n",
        "    # Segment audio for the utterance\n",
        "    start_time = int(utterance.start * 1000)  # convert to milliseconds\n",
        "    end_time = int(utterance.end * 1000)  # convert to milliseconds\n",
        "    segment_audio_path = f\"{speaker_id}_segment.wav\"  # Update to a valid path for each segment\n",
        "\n",
        "    # Extract the segment using librosa and save it\n",
        "    audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
        "    sf.write(segment_audio_path, audio_input, 16000)\n",
        "\n",
        "    # Classify emotion for this segment\n",
        "    emotion_label = classify_emotion(segment_audio_path)\n",
        "\n",
        "    # Store emotion label\n",
        "    speaker_emotions[speaker_id].append(emotion_label)\n",
        "\n",
        "# Print overall sentiment for each speaker\n",
        "print(\"\\nOverall Sentiment Analysis:\")\n",
        "for speaker, sentiments in speaker_sentiments.items():\n",
        "    total = sum(sentiments.values())\n",
        "    overall_sentiment = max(sentiments, key=sentiments.get)\n",
        "    print(f\"{speaker}: {overall_sentiment.capitalize()} (Counts - Positive: {sentiments['positive']}, \"\n",
        "          f\"Negative: {sentiments['negative']}, Neutral: {sentiments['neutral']})\")\n",
        "\n",
        "# Print overall emotion classification for each speaker\n",
        "print(\"\\nOverall Tonal Classification:\")\n",
        "for speaker, emotions in speaker_emotions.items():\n",
        "    emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}\n",
        "    most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "    print(f\"{speaker}: {most_common_emotion} (Counts - {emotion_counts})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcG3qyW_ty2f",
        "outputId": "d008c3b6-ae95-4588-be46-03602db962e4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at superb/wav2vec2-large-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-large-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why. So he called Peter DiCarlo, an associate professor in the department of Environmental Health and Engineering at Johns Hopkins University. Good morning. Professor.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.818103) Timestamp: 240 - 6262\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.5900679) Timestamp: 6446 - 10998\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.537145) Timestamp: 11094 - 15734\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.79514164) Timestamp: 15902 - 18390\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9258168) Timestamp: 18430 - 25334\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 25502 - 26006\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.6240708) Timestamp: 26038 - 26770\n",
            "Speaker B: Good morning.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 27850 - 28786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.7462095) Timestamp: 28938 - 37350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those canadian wildfires through Pennsylvania into the mid Atlantic in the northeast and kind of just dropping the smoke there.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.73488915) Timestamp: 39050 - 40510\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.61490905) Timestamp: 40930 - 56104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is harmful.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.8751137) Timestamp: 56282 - 59284\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.80006605) Timestamp: 59332 - 60920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: It is. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic, smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological, your brain.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62340 - 62884\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62932 - 63332\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9444374) Timestamp: 63396 - 66920\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.7673307) Timestamp: 67740 - 82800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.93296945) Timestamp: 83420 - 85460\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8246211) Timestamp: 85500 - 87964\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8175228) Timestamp: 88012 - 89244\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8153341) Timestamp: 89372 - 90316\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8712161) Timestamp: 90348 - 92400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Yeah. So the concentration of particulate matter I was looking at, some of the monitors that we have was reaching levels of what are in science peak 150 micrograms per meter cubed, which is more than ten times what the annual average should be in about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.54914486) Timestamp: 93460 - 93860\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6478079) Timestamp: 93900 - 113244\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.49572414) Timestamp: 113372 - 119628\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9395254) Timestamp: 119724 - 123380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.53734136) Timestamp: 123460 - 124900\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.779357) Timestamp: 124980 - 128628\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.48532158) Timestamp: 128684 - 132828\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5655255) Timestamp: 132884 - 136360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: It's the youngest. So children, obviously, whose bodies are still developing, the elderly, who are, you know, their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have pre existing health conditions, people with respiratory conditions or heart conditions, can be triggered by high levels of air pollution.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.78433657) Timestamp: 137440 - 138704\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.890515) Timestamp: 138752 - 149620\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8824855) Timestamp: 150560 - 156780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Could this get worse?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.9435272) Timestamp: 157360 - 158860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: That's a good question. I mean, I think if, and in some areas it's much worse than others, and it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.828334) Timestamp: 162120 - 163112\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8275031) Timestamp: 163216 - 170320\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7581229) Timestamp: 171020 - 176892\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7974404) Timestamp: 177076 - 183652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: I was going to ask you about.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: More fires start burning. I don't expect the concentrations to go up too much higher.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.5720193) Timestamp: 185644 - 189060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: I was going to ask you how, and you started to answer this, but how much longer could this last? Forgive me if I'm asking you to speculate, but what do you think?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5477228) Timestamp: 189140 - 193364\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.7086924) Timestamp: 193452 - 196480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Well, I think the fires are going to burn for a little bit longer, but the key for us in the US is the weather system changing. And so right now it's kind of the weather systems that are pulling that air into our mid Atlantic and northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6709003) Timestamp: 198080 - 203720\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8217785) Timestamp: 203760 - 211048\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6655182) Timestamp: 211144 - 219088\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6444754) Timestamp: 219144 - 220952\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8290641) Timestamp: 220976 - 227616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.69644165) Timestamp: 227728 - 232328\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5142844) Timestamp: 232464 - 240100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the western Us. So the eastern us getting hit right now is a little bit new. But, yeah, I think with climate change moving forward, this is something that is going to happen more frequently.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6701288) Timestamp: 241320 - 245080\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.4755036) Timestamp: 245120 - 251136\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6273222) Timestamp: 251168 - 255592\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.58344245) Timestamp: 255696 - 258040\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.66216063) Timestamp: 258160 - 261700\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6574885) Timestamp: 262160 - 267792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: That's Peter DiCarlo, associate professor in the department of Environmental Health and engineering at Johns Hopkins University. Professor de Carlo, thanks so much for joining us and sharing this expertise with us.\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9091453) Timestamp: 267976 - 274576\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.97781634) Timestamp: 274688 - 278460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Thank you for having me.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.97090584) Timestamp: 279440 - 280280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-f5e538abdde1>:125: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Sentiment Analysis:\n",
            "A: Neutral (Counts - Positive: 2, Negative: 9, Neutral: 15)\n",
            "B: Neutral (Counts - Positive: 3, Negative: 7, Neutral: 21)\n",
            "\n",
            "Overall Tonal Classification:\n",
            "A: 2 (Counts - {0: 1, 2: 9})\n",
            "B: 2 (Counts - {2: 10})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Set your API key for AssemblyAI\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a publicly-accessible audio URL or a local file\n",
        "audio_file = \"/content/20230607_me_canadian_wildfires.mp3\"\n",
        "\n",
        "# Configure transcription with speaker labels and sentiment analysis\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        "    sentiment_analysis=True\n",
        ")\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Load the pretrained model and processor for emotion recognition\n",
        "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "emotion_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "\n",
        "# Define emotion labels based on your model's output\n",
        "emotion_labels = {\n",
        "    0: \"neutral\",\n",
        "    1: \"happy\",\n",
        "    2: \"angry\",\n",
        "    3: \"sad\"\n",
        "}\n",
        "\n",
        "# Function to classify emotions from audio\n",
        "def classify_emotion(audio_path):\n",
        "    audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Define a common input length for Wav2Vec2 models, usually 2 seconds at 16kHz\n",
        "    target_length = 32000  # 2 seconds of audio at 16000 Hz\n",
        "\n",
        "    # Check if audio length is less than the target length\n",
        "    if len(audio_input) < target_length:\n",
        "        # Pad the audio with zeros (silence) if it's too short\n",
        "        padding_length = target_length - len(audio_input)\n",
        "        audio_input = np.pad(audio_input, (0, padding_length), 'constant')\n",
        "    else:\n",
        "        # Trim the audio if it's too long\n",
        "        audio_input = audio_input[:target_length]\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    # Get emotion prediction\n",
        "    with torch.no_grad():\n",
        "        logits = emotion_model(**inputs).logits\n",
        "\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Map the predicted ID to the corresponding emotion label\n",
        "    emotion_label = emotion_labels.get(predicted_id, \"unknown\")\n",
        "\n",
        "    return emotion_label\n",
        "\n",
        "# Initialize sentiment counters and emotion lists for each speaker\n",
        "speaker_sentiments = {}\n",
        "speaker_emotions = {}\n",
        "\n",
        "# Process each utterance in the transcription\n",
        "for utterance in transcript.utterances:\n",
        "    speaker_id = utterance.speaker  # e.g., 'A' or 'B'\n",
        "\n",
        "    # Initialize counters and lists for the speaker if not already present\n",
        "    if speaker_id not in speaker_sentiments:\n",
        "        speaker_sentiments[speaker_id] = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "        speaker_emotions[speaker_id] = []\n",
        "\n",
        "    # Print speaker and their spoken text\n",
        "    print(f\"Speaker {speaker_id}: {utterance.text}\")\n",
        "\n",
        "    # Extract sentiment analysis for each utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            sentiment = sentiment_result.sentiment\n",
        "\n",
        "            # Update sentiment counters for the corresponding speaker\n",
        "            if sentiment == \"POSITIVE\":\n",
        "                speaker_sentiments[speaker_id]['positive'] += 1\n",
        "            elif sentiment == \"NEGATIVE\":\n",
        "                speaker_sentiments[speaker_id]['negative'] += 1\n",
        "            elif sentiment == \"NEUTRAL\":\n",
        "                speaker_sentiments[speaker_id]['neutral'] += 1\n",
        "\n",
        "            print(f\"Speaker {speaker_id} - Sentiment: {sentiment} \"\n",
        "                  f\"(Confidence: {sentiment_result.confidence}) \"\n",
        "                  f\"Timestamp: {sentiment_result.start} - {sentiment_result.end}\")\n",
        "\n",
        "    # Segment audio for the utterance\n",
        "    start_time = utterance.start  # in seconds\n",
        "    end_time = utterance.end  # in seconds\n",
        "    segment_audio_path = f\"{speaker_id}_segment_{start_time}.wav\"  # Unique file name\n",
        "\n",
        "    # Extract the audio segment for this utterance\n",
        "    audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
        "    sf.write(segment_audio_path, audio_input, 16000)\n",
        "\n",
        "    # Classify emotion for this segment\n",
        "    emotion_label = classify_emotion(segment_audio_path)\n",
        "\n",
        "    # Store the emotion label for the speaker\n",
        "    speaker_emotions[speaker_id].append(emotion_label)\n",
        "\n",
        "# Print overall sentiment analysis for each speaker\n",
        "print(\"\\nOverall Sentiment Analysis:\")\n",
        "for speaker, sentiments in speaker_sentiments.items():\n",
        "    total = sum(sentiments.values())\n",
        "    overall_sentiment = max(sentiments, key=sentiments.get)\n",
        "    print(f\"{speaker}: {overall_sentiment.capitalize()} (Counts - Positive: {sentiments['positive']}, \"\n",
        "          f\"Negative: {sentiments['negative']}, Neutral: {sentiments['neutral']})\")\n",
        "\n",
        "# Print overall emotion classification for each speaker\n",
        "print(\"\\nOverall Tonal Classification:\")\n",
        "for speaker, emotions in speaker_emotions.items():\n",
        "    emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}\n",
        "    most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "    print(f\"{speaker}: {most_common_emotion} (Counts - {emotion_counts})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PJPyu8Yw85P",
        "outputId": "cf197b17-dac1-45e3-e5e1-a40b67bad2d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at superb/wav2vec2-large-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-large-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why. So he called Peter DiCarlo, an associate professor in the department of Environmental Health and Engineering at Johns Hopkins University. Good morning. Professor.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.818103) Timestamp: 240 - 6262\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.5900679) Timestamp: 6446 - 10998\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.537145) Timestamp: 11094 - 15734\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.79514164) Timestamp: 15902 - 18390\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9258168) Timestamp: 18430 - 25334\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 25502 - 26006\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.6240708) Timestamp: 26038 - 26770\n",
            "Speaker B: Good morning.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 27850 - 28786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.7462095) Timestamp: 28938 - 37350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those canadian wildfires through Pennsylvania into the mid Atlantic in the northeast and kind of just dropping the smoke there.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.73488915) Timestamp: 39050 - 40510\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.61490905) Timestamp: 40930 - 56104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is harmful.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.8751137) Timestamp: 56282 - 59284\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.80006605) Timestamp: 59332 - 60920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: It is. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic, smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological, your brain.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62340 - 62884\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62932 - 63332\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9444374) Timestamp: 63396 - 66920\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.7673307) Timestamp: 67740 - 82800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.93296945) Timestamp: 83420 - 85460\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8246211) Timestamp: 85500 - 87964\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8175228) Timestamp: 88012 - 89244\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8153341) Timestamp: 89372 - 90316\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8712161) Timestamp: 90348 - 92400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Yeah. So the concentration of particulate matter I was looking at, some of the monitors that we have was reaching levels of what are in science peak 150 micrograms per meter cubed, which is more than ten times what the annual average should be in about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.54914486) Timestamp: 93460 - 93860\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6478079) Timestamp: 93900 - 113244\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.49572414) Timestamp: 113372 - 119628\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9395254) Timestamp: 119724 - 123380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.53734136) Timestamp: 123460 - 124900\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.779357) Timestamp: 124980 - 128628\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.48532158) Timestamp: 128684 - 132828\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5655255) Timestamp: 132884 - 136360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: It's the youngest. So children, obviously, whose bodies are still developing, the elderly, who are, you know, their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have pre existing health conditions, people with respiratory conditions or heart conditions, can be triggered by high levels of air pollution.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.78433657) Timestamp: 137440 - 138704\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.890515) Timestamp: 138752 - 149620\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8824855) Timestamp: 150560 - 156780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Could this get worse?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.9435272) Timestamp: 157360 - 158860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: That's a good question. I mean, I think if, and in some areas it's much worse than others, and it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.828334) Timestamp: 162120 - 163112\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8275031) Timestamp: 163216 - 170320\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7581229) Timestamp: 171020 - 176892\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7974404) Timestamp: 177076 - 183652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: I was going to ask you about.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: More fires start burning. I don't expect the concentrations to go up too much higher.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.5720193) Timestamp: 185644 - 189060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: I was going to ask you how, and you started to answer this, but how much longer could this last? Forgive me if I'm asking you to speculate, but what do you think?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5477228) Timestamp: 189140 - 193364\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.7086924) Timestamp: 193452 - 196480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Well, I think the fires are going to burn for a little bit longer, but the key for us in the US is the weather system changing. And so right now it's kind of the weather systems that are pulling that air into our mid Atlantic and northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6709003) Timestamp: 198080 - 203720\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8217785) Timestamp: 203760 - 211048\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6655182) Timestamp: 211144 - 219088\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6444754) Timestamp: 219144 - 220952\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8290641) Timestamp: 220976 - 227616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.69644165) Timestamp: 227728 - 232328\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5142844) Timestamp: 232464 - 240100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: That's Peter DiCarlo, associate professor in the department of Environmental Health and engineering at Johns Hopkins University. Professor de Carlo, thanks so much for joining us and sharing this expertise with us.\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9091453) Timestamp: 267976 - 274576\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.97781634) Timestamp: 274688 - 278460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B: Thank you for having me.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.97090584) Timestamp: 279440 - 280280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1811ccfe674>:104: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Sentiment Analysis:\n",
            "A: Neutral (Counts - Positive: 2, Negative: 9, Neutral: 15)\n",
            "B: Neutral (Counts - Positive: 3, Negative: 7, Neutral: 21)\n",
            "\n",
            "Overall Tonal Classification:\n",
            "A: angry (Counts - {'angry': 9, 'neutral': 1})\n",
            "B: angry (Counts - {'angry': 10})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's configuration\n",
        "model_config = emotion_model.config\n",
        "\n",
        "# Retrieve the label-to-emotion mapping\n",
        "id2label = model_config.id2label\n",
        "\n",
        "# Print the mapping\n",
        "print(\"Emotion ID to Label Mapping:\")\n",
        "print(id2label)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U23ATIKv1rNn",
        "outputId": "8ee3d618-1c01-4133-f8b3-b4b3093e4f4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion ID to Label Mapping:\n",
            "{0: 'neu', 1: 'hap', 2: 'ang', 3: 'sad'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without transcription\n",
        "import assemblyai as aai\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Set your AssemblyAI API key for speaker diarization and sentiment analysis\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Load your audio file\n",
        "audio_file = \"/content/20230607_me_canadian_wildfires.mp3\"\n",
        "\n",
        "# Configure diarization and sentiment analysis (without transcription)\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,  # For speaker diarization\n",
        "    sentiment_analysis=True  # For sentiment analysis\n",
        ")\n",
        "\n",
        "# Perform speaker diarization and sentiment analysis using AssemblyAI\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Load the pretrained model and processor for emotion recognition\n",
        "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "emotion_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "\n",
        "# Define the function for classifying emotions from audio segments\n",
        "def classify_emotion(audio_path):\n",
        "    audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Define a common input length for Wav2Vec2 models (e.g., 2 seconds)\n",
        "    target_length = 32000  # 2 seconds of audio at 16000 Hz\n",
        "\n",
        "    # Pad or trim the audio to fit the target length\n",
        "    if len(audio_input) < target_length:\n",
        "        padding_length = target_length - len(audio_input)\n",
        "        audio_input = np.pad(audio_input, (0, padding_length), 'constant')\n",
        "    else:\n",
        "        audio_input = audio_input[:target_length]\n",
        "\n",
        "    # Prepare inputs for the emotion model\n",
        "    inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = emotion_model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    return predicted_ids.item()\n",
        "\n",
        "# Initialize sentiment counters for each speaker\n",
        "speaker_sentiments = {}\n",
        "speaker_emotions = {}\n",
        "\n",
        "# Process speaker diarization and sentiment analysis\n",
        "for utterance in transcript.utterances:\n",
        "    speaker_id = utterance.speaker  # Get speaker ID\n",
        "\n",
        "    # Initialize sentiment and emotion tracking for this speaker\n",
        "    if speaker_id not in speaker_sentiments:\n",
        "        speaker_sentiments[speaker_id] = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "        speaker_emotions[speaker_id] = []\n",
        "\n",
        "    # Handle sentiment analysis for this utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            sentiment = sentiment_result.sentiment\n",
        "\n",
        "            # Update sentiment counts\n",
        "            if sentiment == \"POSITIVE\":\n",
        "                speaker_sentiments[speaker_id]['positive'] += 1\n",
        "            elif sentiment == \"NEGATIVE\":\n",
        "                speaker_sentiments[speaker_id]['negative'] += 1\n",
        "            elif sentiment == \"NEUTRAL\":\n",
        "                speaker_sentiments[speaker_id]['neutral'] += 1\n",
        "\n",
        "    # Perform emotion classification on the audio segment\n",
        "    start_time = int(utterance.start * 1000)  # Convert to milliseconds\n",
        "    end_time = int(utterance.end * 1000)\n",
        "\n",
        "    segment_audio_path = f\"{speaker_id}_segment.wav\"  # Path for audio segment\n",
        "\n",
        "    # Extract the audio segment using librosa\n",
        "    audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
        "    sf.write(segment_audio_path, audio_input, 16000)\n",
        "\n",
        "    # Classify the emotion for this audio segment\n",
        "    emotion_label = classify_emotion(segment_audio_path)\n",
        "\n",
        "    # Store the classified emotion for the speaker\n",
        "    speaker_emotions[speaker_id].append(emotion_label)\n",
        "\n",
        "# Print overall sentiment and emotion analysis\n",
        "print(\"\\nOverall Sentiment Analysis:\")\n",
        "for speaker, sentiments in speaker_sentiments.items():\n",
        "    overall_sentiment = max(sentiments, key=sentiments.get)\n",
        "    print(f\"Speaker {speaker}: {overall_sentiment.capitalize()}\")\n",
        "\n",
        "# print(\"\\nOverall Tonal Classification:\")\n",
        "# for speaker, emotions in speaker_emotions.items():\n",
        "#     emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}\n",
        "#     most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "#     print(f\"Speaker {speaker}: {most_common_emotion} (Counts: {emotion_counts})\")\n",
        "\n",
        "# Print overall tonal classification with emotion labels\n",
        "print(\"\\nOverall Tonal Classification:\")\n",
        "for speaker, emotions in speaker_emotions.items():\n",
        "    emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}\n",
        "    most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "\n",
        "    # Convert emotion ID to label\n",
        "    emotion_label = id2label[most_common_emotion]\n",
        "\n",
        "    # Print results with emotion labels instead of numbers\n",
        "    print(f\"Speaker {speaker}: {emotion_label} (Counts: {emotion_counts})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZuge5aR1wja",
        "outputId": "f6552338-026e-4488-f2f8-2f37f88ea1f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at superb/wav2vec2-large-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-large-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-11-3544ad34b19c>:83: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time / 1000, duration=(end_time - start_time) / 1000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Sentiment Analysis:\n",
            "Speaker A: Neutral\n",
            "Speaker B: Neutral\n",
            "\n",
            "Overall Tonal Classification:\n",
            "Speaker A: ang (Counts: {0: 1, 2: 9})\n",
            "Speaker B: ang (Counts: {2: 10})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "import assemblyai as aai\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Set your API key for AssemblyAI\n",
        "aai.settings.api_key = \"f69817768510455f96f4088d628065fb\"\n",
        "\n",
        "# Use a publicly-accessible audio URL or a local file\n",
        "audio_file = \"/content/20230607_me_canadian_wildfires.mp3\"\n",
        "\n",
        "# Configure transcription with speaker labels and sentiment analysis\n",
        "config = aai.TranscriptionConfig(\n",
        "    speaker_labels=True,\n",
        "    sentiment_analysis=True\n",
        ")\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcript = aai.Transcriber().transcribe(audio_file, config)\n",
        "\n",
        "# Load the pretrained model and processor for emotion recognition\n",
        "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "emotion_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
        "\n",
        "# Define emotion labels based on your model's output\n",
        "emotion_labels = {\n",
        "    0: \"neutral\",\n",
        "    1: \"happy\",\n",
        "    2: \"angry\",\n",
        "    3: \"sad\"\n",
        "}\n",
        "\n",
        "# Function to classify emotions from audio\n",
        "def classify_emotion(audio_path):\n",
        "    audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Define a common input length for Wav2Vec2 models, usually 2 seconds at 16kHz\n",
        "    target_length = 32000  # 2 seconds of audio at 16000 Hz\n",
        "\n",
        "    # Check if audio length is less than the target length\n",
        "    if len(audio_input) < target_length:\n",
        "        # Pad the audio with zeros (silence) if it's too short\n",
        "        padding_length = target_length - len(audio_input)\n",
        "        audio_input = np.pad(audio_input, (0, padding_length), 'constant')\n",
        "    else:\n",
        "        # Trim the audio if it's too long\n",
        "        audio_input = audio_input[:target_length]\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    inputs = emotion_processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    # Get emotion prediction\n",
        "    with torch.no_grad():\n",
        "        logits = emotion_model(**inputs).logits\n",
        "\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Map the predicted ID to the corresponding emotion label\n",
        "    emotion_label = emotion_labels.get(predicted_id, \"unknown\")\n",
        "\n",
        "    return emotion_label\n",
        "\n",
        "# Initialize sentiment counters and emotion lists for each speaker\n",
        "speaker_sentiments = {}\n",
        "speaker_emotions = {}\n",
        "\n",
        "# Process each utterance in the transcription\n",
        "for utterance in transcript.utterances:\n",
        "    speaker_id = utterance.speaker  # e.g., 'A' or 'B'\n",
        "\n",
        "    # Initialize counters and lists for the speaker if not already present\n",
        "    if speaker_id not in speaker_sentiments:\n",
        "        speaker_sentiments[speaker_id] = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "        speaker_emotions[speaker_id] = []\n",
        "\n",
        "    # Print speaker and their spoken text\n",
        "    print(f\"Speaker {speaker_id}: {utterance.text}\")\n",
        "\n",
        "    # Extract sentiment analysis for each utterance\n",
        "    for sentiment_result in transcript.sentiment_analysis:\n",
        "        if sentiment_result.start >= utterance.start and sentiment_result.end <= utterance.end:\n",
        "            sentiment = sentiment_result.sentiment\n",
        "\n",
        "            # Update sentiment counters for the corresponding speaker\n",
        "            if sentiment == \"POSITIVE\":\n",
        "                speaker_sentiments[speaker_id]['positive'] += 1\n",
        "            elif sentiment == \"NEGATIVE\":\n",
        "                speaker_sentiments[speaker_id]['negative'] += 1\n",
        "            elif sentiment == \"NEUTRAL\":\n",
        "                speaker_sentiments[speaker_id]['neutral'] += 1\n",
        "\n",
        "            print(f\"Speaker {speaker_id} - Sentiment: {sentiment} \"\n",
        "                  f\"(Confidence: {sentiment_result.confidence}) \"\n",
        "                  f\"Timestamp: {sentiment_result.start} - {sentiment_result.end}\")\n",
        "\n",
        "    # Segment audio for the utterance and classify emotion for this segment\n",
        "    start_time = utterance.start  # in seconds\n",
        "    end_time = utterance.end  # in seconds\n",
        "    segment_audio_path = f\"{speaker_id}_segment_{start_time}.wav\"  # Unique file name\n",
        "\n",
        "    # Extract the audio segment for this utterance\n",
        "    audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
        "    sf.write(segment_audio_path, audio_input, 16000)\n",
        "\n",
        "    # Classify emotion for this segment\n",
        "    emotion_label = classify_emotion(segment_audio_path)\n",
        "\n",
        "    # Print emotion label for each utterance\n",
        "    print(f\"Speaker {speaker_id} - Emotion: {emotion_label} (Segment: {start_time}s - {end_time}s)\")\n",
        "\n",
        "    # Store the emotion label for the speaker\n",
        "    speaker_emotions[speaker_id].append(emotion_label)\n",
        "\n",
        "# Print overall sentiment analysis for each speaker\n",
        "print(\"\\nOverall Sentiment Analysis:\")\n",
        "for speaker, sentiments in speaker_sentiments.items():\n",
        "    total = sum(sentiments.values())\n",
        "    overall_sentiment = max(sentiments, key=sentiments.get)\n",
        "    print(f\"{speaker}: {overall_sentiment.capitalize()} (Counts - Positive: {sentiments['positive']}, \"\n",
        "          f\"Negative: {sentiments['negative']}, Neutral: {sentiments['neutral']})\")\n",
        "\n",
        "# Print overall emotion classification for each speaker\n",
        "print(\"\\nOverall Tonal Classification:\")\n",
        "for speaker, emotions in speaker_emotions.items():\n",
        "    emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}\n",
        "    most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "    print(f\"{speaker}: {most_common_emotion} (Counts - {emotion_counts})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCu4kqQ7-geT",
        "outputId": "c3fcf927-3ccc-46d3-f5fe-c2c384697f93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at superb/wav2vec2-large-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-large-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why. So he called Peter DiCarlo, an associate professor in the department of Environmental Health and Engineering at Johns Hopkins University. Good morning. Professor.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.818103) Timestamp: 240 - 6262\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.5900679) Timestamp: 6446 - 10998\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.537145) Timestamp: 11094 - 15734\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.79514164) Timestamp: 15902 - 18390\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9258168) Timestamp: 18430 - 25334\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 25502 - 26006\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.6240708) Timestamp: 26038 - 26770\n",
            "Speaker A - Emotion: neutral (Segment: 240s - 26770s)\n",
            "Speaker B: Good morning.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.7193592) Timestamp: 27850 - 28786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 27850s - 28786s)\n",
            "Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.7462095) Timestamp: 28938 - 37350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 28938s - 37350s)\n",
            "Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those canadian wildfires through Pennsylvania into the mid Atlantic in the northeast and kind of just dropping the smoke there.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.73488915) Timestamp: 39050 - 40510\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.61490905) Timestamp: 40930 - 56104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 39050s - 56104s)\n",
            "Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is harmful.\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.8751137) Timestamp: 56282 - 59284\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.80006605) Timestamp: 59332 - 60920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 56282s - 60920s)\n",
            "Speaker B: It is. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic, smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological, your brain.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62340 - 62884\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.60547477) Timestamp: 62932 - 63332\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9444374) Timestamp: 63396 - 66920\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.7673307) Timestamp: 67740 - 82800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 62340s - 82800s)\n",
            "Speaker A: What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.93296945) Timestamp: 83420 - 85460\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8246211) Timestamp: 85500 - 87964\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8175228) Timestamp: 88012 - 89244\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8153341) Timestamp: 89372 - 90316\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.8712161) Timestamp: 90348 - 92400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 83420s - 92400s)\n",
            "Speaker B: Yeah. So the concentration of particulate matter I was looking at, some of the monitors that we have was reaching levels of what are in science peak 150 micrograms per meter cubed, which is more than ten times what the annual average should be in about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.54914486) Timestamp: 93460 - 93860\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6478079) Timestamp: 93900 - 113244\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.49572414) Timestamp: 113372 - 119628\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.9395254) Timestamp: 119724 - 123380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 93460s - 123380s)\n",
            "Speaker A: And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.53734136) Timestamp: 123460 - 124900\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.779357) Timestamp: 124980 - 128628\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.48532158) Timestamp: 128684 - 132828\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5655255) Timestamp: 132884 - 136360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 123460s - 136360s)\n",
            "Speaker B: It's the youngest. So children, obviously, whose bodies are still developing, the elderly, who are, you know, their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have pre existing health conditions, people with respiratory conditions or heart conditions, can be triggered by high levels of air pollution.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.78433657) Timestamp: 137440 - 138704\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.890515) Timestamp: 138752 - 149620\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8824855) Timestamp: 150560 - 156780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 137440s - 156780s)\n",
            "Speaker A: Could this get worse?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.9435272) Timestamp: 157360 - 158860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 157360s - 158860s)\n",
            "Speaker B: That's a good question. I mean, I think if, and in some areas it's much worse than others, and it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.828334) Timestamp: 162120 - 163112\n",
            "Speaker B - Sentiment: NEGATIVE (Confidence: 0.8275031) Timestamp: 163216 - 170320\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7581229) Timestamp: 171020 - 176892\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.7974404) Timestamp: 177076 - 183652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 162120s - 183652s)\n",
            "Speaker A: I was going to ask you about.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 183796s - 184620s)\n",
            "Speaker B: More fires start burning. I don't expect the concentrations to go up too much higher.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.5720193) Timestamp: 185644 - 189060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 184660s - 189060s)\n",
            "Speaker A: I was going to ask you how, and you started to answer this, but how much longer could this last? Forgive me if I'm asking you to speculate, but what do you think?\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5477228) Timestamp: 189140 - 193364\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.7086924) Timestamp: 193452 - 196480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 189140s - 196480s)\n",
            "Speaker B: Well, I think the fires are going to burn for a little bit longer, but the key for us in the US is the weather system changing. And so right now it's kind of the weather systems that are pulling that air into our mid Atlantic and northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6709003) Timestamp: 198080 - 203720\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8217785) Timestamp: 203760 - 211048\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6655182) Timestamp: 211144 - 219088\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6444754) Timestamp: 219144 - 220952\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.8290641) Timestamp: 220976 - 227616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 198080s - 227616s)\n",
            "Speaker A: And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?\n",
            "Speaker A - Sentiment: NEGATIVE (Confidence: 0.69644165) Timestamp: 227728 - 232328\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.5142844) Timestamp: 232464 - 240100\n",
            "Speaker A - Emotion: angry (Segment: 227728s - 240100s)\n",
            "Speaker B: I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the western Us. So the eastern us getting hit right now is a little bit new. But, yeah, I think with climate change moving forward, this is something that is going to happen more frequently.\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6701288) Timestamp: 241320 - 245080\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.4755036) Timestamp: 245120 - 251136\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6273222) Timestamp: 251168 - 255592\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.58344245) Timestamp: 255696 - 258040\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.66216063) Timestamp: 258160 - 261700\n",
            "Speaker B - Sentiment: NEUTRAL (Confidence: 0.6574885) Timestamp: 262160 - 267792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 241320s - 267792s)\n",
            "Speaker A: That's Peter DiCarlo, associate professor in the department of Environmental Health and engineering at Johns Hopkins University. Professor de Carlo, thanks so much for joining us and sharing this expertise with us.\n",
            "Speaker A - Sentiment: NEUTRAL (Confidence: 0.9091453) Timestamp: 267976 - 274576\n",
            "Speaker A - Sentiment: POSITIVE (Confidence: 0.97781634) Timestamp: 274688 - 278460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker A - Emotion: angry (Segment: 267976s - 278460s)\n",
            "Speaker B: Thank you for having me.\n",
            "Speaker B - Sentiment: POSITIVE (Confidence: 0.97090584) Timestamp: 279440 - 280280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8073865f900c>:106: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, _ = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time - start_time)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker B - Emotion: angry (Segment: 279440s - 280280s)\n",
            "\n",
            "Overall Sentiment Analysis:\n",
            "A: Neutral (Counts - Positive: 2, Negative: 9, Neutral: 15)\n",
            "B: Neutral (Counts - Positive: 3, Negative: 7, Neutral: 21)\n",
            "\n",
            "Overall Tonal Classification:\n",
            "A: angry (Counts - {'angry': 9, 'neutral': 1})\n",
            "B: angry (Counts - {'angry': 10})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cx_kfaEK8Xpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
